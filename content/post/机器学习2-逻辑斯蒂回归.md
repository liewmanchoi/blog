---
title: "机器学习2-逻辑斯蒂回归"
date: 2018-10-23T11:06:48+08:00
draft: false
lastmod: 2018-10-23T11:06:48+08:00
tags: ["算法工程师", "机器学习"]
categories: ["机器学习"]
author: "王圣"
---

### 1. 模型假设
* 假设现有如下的训练数据
$$ D = {(\mathbf{x_1}, y_1), (\mathbf{x_2}, y_2), ..., (\mathbf{x_n}, y_n)}$$
其中，$\mathbf{x_i}$是m维的向量，即
$$\mathbf{x_i} = (x_i^{(1)}, x_i^{(2)}, ..., x_i^{(m)})$$
$y_i$的取值为1或0，分别代表正例和负例

* 要求求得分类决策函数$$f(x) = y^{*}$$  
	能在未知数据集上有良好的分类表现

### 2. 模型推导
* sigmoid函数：sigmoid函数的取值分布在\[0, 1]之间，而且在远离0的地方会很快趋近于0/1。  
表达式为  
$$ g(x) = \frac{1}{1+exp(-x)}$$
函数图像为 
<center> 
<img src="/images/sigmoid函数.png" width=256 height=256 />  
sigmoid函数图像
<center />

* 二项逻辑斯蒂回归模型：
	* 条件概率$P(y|x)$的取值如下：
	
	$$\begin{align} P(Y=1|x) &= \frac{1}{1+exp(-(w \cdot x + b))} = \frac{exp(w \cdot x + b)}{1 + exp(w \cdot x + b)}   \\\\\\
	P(Y=0|x) &= 1- P(Y=1|x)
	\end{align}$$

	其中，$x \in R^m$是输入，$Y \in {\{0, 1\}}$是输出，参数$w \in R^m$称为权值向量，$b \in R$称为偏置。为了表述方便，将权值向量和输入向量进行扩充，即$$\begin{align}
	\mathbf{w} &=(w^{(1)}, w^{(2)}, ..., w^{(m)}, b)^T\\\\\\
	\mathbf{x} &=(x^{(1)}, x^{(2)}, ..., x^{(m)}, 1)^T\end{align}\\\\\\
	\therefore P(Y=1|x) = \frac{1}{1+exp(-(w \cdot x))} = \frac{exp(w \cdot x)}{1 + exp(w \cdot x)}$$  

	* LR将较大的条件概率值对应的分类作为预测输出
	* 对数几率：如果事件发生的概率为*p*，那么该事件的几率为$\frac{p}{1-p}$，对数几率为$$logit(p)=log\frac{p}{1-p}$$
	对逻辑斯蒂回归而言，
	$$log\frac{P(y=1|x)}{1-P(y=1|x)} = log\frac{P(y=1|x)}{P(y=0|x)}=w \cdot x$$
	所以，输出$y=1$的对数几率是输入$x$的线性函数，逻辑斯蒂回归实际上是在使用线性模型的预测结果去逼近真实标记的对数几率

* 逻辑斯蒂回归模型参数估计：极大似然估计法
	* 变换：设
	$$\begin{align} &P(Y=1|x)=\pi(x), P(Y=0|x) = 1- \pi(x)\\\\\\
	&\because y_i \in \{0, 1\}\\\\\\
	&\therefore P(y_i|x_i) = \color{blue}{(\pi(x_i))^{y_i}(1-\pi(x_i))^{(1-y_i)}} \end{align}$$
	* 似然函数为$$\prod_{i=1}^n \color{blue}{(\pi(x_i))^{y_i}(1-\pi(x_i))^{(1-y_i)}}$$
	* 对数似然函数为
	$$\begin{align} L(w) &= \sum_{i=1}^n(y_i log(\pi(x\_i)) + (1-y_i)log(1-\pi(x\_i)))\\\\\\
	&= \sum\_{i=1}^n (yw^T x\_i - log(1+ exp(w^T x\_i))) \end{align}$$
	
	
	**最大化**似然函数$\max L(w)$应当转换为**最小化**形式
	$$ \color{red}{w^{* } = \underset{w}{\arg \min} \sum_{i=1}^n (-yw^T x_i + log(1+ exp(w^T x_i)))}$$
	
	
	也可以考虑加入正则项，则损失函数变成了
	
	$$J(w) = \sum_{i=1}^n(-y_i log(\pi(x\_i)) - (1-y\_i)log(1-\pi(x\_i))) + \lambda {\left \|| w \right \||}\_{p}$$
	
	对数似然函数求导得到梯度
	$$\triangledown J(w) = \frac{\partial J(w)}{\partial w} = \color{red}{\sum_{i=1}^n (\pi(x_i)-y_i)x_i}$$ 
	使用随机梯度下降法或者拟牛顿法即可求得符合条件的最优参数估计$w^{* }$
	
### 3. 多元逻辑回归
* softmax函数
	$$S_i = \frac{exp(V_i)}{\sum_i^C exp(V_i)}$$

* 多元逻辑回归模型

	$$P(y=i|x\_i, w) = \frac{exp(w^T x\_i)} {\sum_{j=1}^K exp(w^T x\_j)}$$


对应的决策函数为$$y^* = \underset{i}{\arg \max} P(y=i|x, w^* )$$
	
### 4. 为什么逻辑斯蒂回归可行？
* LR是一种判别模型，表现在直接对条件概率$P(y|x)$建模，而不关心联合概率分布$P(x, y)$
* 特征数据的分布$P(y|x)$通常满足高斯分布，考虑二分类问题，$y=1$的对数几率为
$$\begin{align}
log \frac{P(y=1|x)}{P(y=0|x)} &= log \frac{\frac{P(y=1, x)}{P(x)}}{\frac{P(y=0, x)}{P(x)}} = log \frac{P(x, y=1)}{P(x, y=0)}\\\\\\
&= log \frac{P(x|y=1)P(y=1)}{P(x|y=0)P(y=0)}\\\\\\
&= log \frac{P(x|y=1)}{P(x|y=0)} + log \frac{P(y=1)}{P(y=0)}\\\\\\
&= - \frac{(x - \mu_1)^2}{2 \sigma_1^2} + \frac{(x - \mu_0)^2}{2 \sigma_0^2} + C
\end{align}$$
如果$\sigma_0 = \sigma_1$，则二次项抵消，得到一个简单的线性关系，即
$$log \frac{P(y=1|x)}{P(y=0|x)} = w^Tx\\\\\\
\therefore P(y=1|x) = \frac{exp(w \cdot x)}{1 + exp(w \cdot x)}$$